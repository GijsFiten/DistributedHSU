{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from lib.coco_eval import CocoEvaluator\n",
    "import random\n",
    "from PIL import Image\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "#Load the category names\n",
    "NYU40CLASSES = ['void',\n",
    "                'wall', 'floor', 'cabinet', 'bed', 'chair',\n",
    "                'sofa', 'table', 'door', 'window', 'bookshelf',\n",
    "                'picture', 'counter', 'blinds', 'desk', 'shelves',\n",
    "                'curtain', 'dresser', 'pillow', 'mirror', 'floor_mat',\n",
    "                'clothes', 'ceiling', 'books', 'refridgerator', 'television',\n",
    "                'paper', 'towel', 'shower_curtain', 'box', 'whiteboard',\n",
    "                'person', 'night_stand', 'toilet', 'sink', 'lamp',\n",
    "                'bathtub', 'bag', 'otherstructure', 'otherfurniture', 'otherprop']\n",
    "\n",
    "# Create a dictionary that maps category IDs to category names\n",
    "id_to_name = {i: name for i, name in enumerate(NYU40CLASSES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, transform_target, data_list):\n",
    "        self.data_list = data_list\n",
    "        self.transform_target = transform_target\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_list[idx]\n",
    "\n",
    "        img = Image.open(data['image_path'])\n",
    "        convert_tensor = T.ToTensor()\n",
    "        img = convert_tensor(img)\n",
    "\n",
    "        boxes = torch.tensor(data['boxes'], dtype=torch.float32)\n",
    "        labels = torch.tensor(data['labels'], dtype=torch.int64)\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((len(data['labels']),), dtype=torch.int64)\n",
    "        area = []\n",
    "        for i in data['boxes']:\n",
    "            area.append((i[2]-i[0])*(i[3]-i[1]))\n",
    "\n",
    "        area = torch.tensor(area,dtype=torch.float32)    \n",
    "\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id' : idx,\n",
    "            'iscrowd' : iscrowd,\n",
    "            'area' : area\n",
    "        }\n",
    "        \n",
    "        target = self.transform_target(target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list) \n",
    "    \n",
    "def arrayFromSUNRGBD(pathToJSON):\n",
    "    cocoDict = json.load(open(pathToJSON))\n",
    "    \n",
    "    listDict = []\n",
    "    # Create a dictionary mapping image IDs to file names\n",
    "    image_paths = {image[\"id\"]: image[\"file_name\"] for image in cocoDict[\"images\"]}\n",
    "    \n",
    "    # Group annotations by image ID\n",
    "    annotations_by_image = {}\n",
    "    for annotation in cocoDict[\"annotations\"]:\n",
    "        if annotation[\"image_id\"] not in annotations_by_image:\n",
    "            annotations_by_image[annotation[\"image_id\"]] = []\n",
    "        annotations_by_image[annotation[\"image_id\"]].append(annotation)\n",
    "    \n",
    "    for i in image_paths.keys():\n",
    "        if i in annotations_by_image:\n",
    "            data_sample = {}\n",
    "            data_sample[\"image_path\"] = image_paths[i]\n",
    "            data_sample[\"boxes\"] = [[annotation[\"bbox\"][0], annotation[\"bbox\"][1], annotation[\"bbox\"][2] + annotation[\"bbox\"][0], annotation[\"bbox\"][3] + annotation[\"bbox\"][1]] for annotation in annotations_by_image[i]]\n",
    "            data_sample[\"labels\"] = [annotation[\"category_id\"] for annotation in annotations_by_image[i]]\n",
    "            listDict.append(data_sample)\n",
    "    \n",
    "    return listDict   \n",
    "\n",
    "def get_transform(train=True):\n",
    "    image_transforms = []\n",
    "    target_transforms = []\n",
    "    if train:\n",
    "        image_transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
    "    image_transforms.append(T.ConvertImageDtype(torch.float32))\n",
    "    image_transforms.append(T.ToTensor())\n",
    "    return T.Compose(image_transforms), T.Compose(target_transforms)\n",
    "\n",
    "\n",
    "def calculatemAP(coco_evaluation):\n",
    "    list_eval = coco_evaluation.coco_eval['bbox'].stats.tolist()\n",
    "    ap_values = []\n",
    "    ap_values.append(list_eval[0])\n",
    "    ap_values.append(list_eval[3])\n",
    "    ap_values.append(list_eval[4])\n",
    "    ap_values.append(list_eval[5])\n",
    "    mAP = np.average(ap_values)\n",
    "    return mAP\n",
    "\n",
    "def calculatemAR(coco_evaluation):\n",
    "    list_eval = coco_evaluation.coco_eval['bbox'].stats.tolist()\n",
    "    ap_values = []\n",
    "    ap_values.append(list_eval[6])\n",
    "    ap_values.append(list_eval[7])\n",
    "    ap_values.append(list_eval[8])\n",
    "    ap_values.append(list_eval[9])\n",
    "    ap_values.append(list_eval[10])\n",
    "    ap_values.append(list_eval[11])\n",
    "    mAR = np.average(ap_values)\n",
    "    return mAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_resnet(num_classes, state_dict):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(state_dict))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_model_mobilenet(num_classes, state_dict):\n",
    "    model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(state_dict))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_model_detr(state_dict):\n",
    "    # Load the pre-trained model with the original number of classes\n",
    "    model = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=False)\n",
    "\n",
    "    # Define a new classification head\n",
    "    num_in_features = model.class_embed.in_features\n",
    "    num_output_features = len(NYU40CLASSES) + 1 # Define your number of output features here\n",
    "\n",
    "    # Replace the classification head\n",
    "    model.class_embed = torch.nn.Linear(num_in_features, num_output_features)\n",
    "    model.load_state_dict(torch.load(state_dict))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "        (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def detect(im, model, transform):\n",
    "    # mean-std normalize the input image (batch-size: 1)\n",
    "    im = to_pil_image(im)\n",
    "    img = transform(im).unsqueeze(0)\n",
    "\n",
    "    # demo model only support by default images with aspect ratio between 0.5 and 2\n",
    "    # if you want to use images with an aspect ratio outside this range\n",
    "    # rescale your image so that the maximum size is at most 1333 for best results\n",
    "    assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'\n",
    "\n",
    "    # propagate through the model\n",
    "    outputs = model(img)\n",
    "\n",
    "    # keep only predictions with 0.7+ confidence\n",
    "    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > 0.9\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "    \n",
    "    predictions = []\n",
    "    for p, (xmin, ymin, xmax, ymax) in zip(probas, bboxes_scaled):\n",
    "        prediction = {\n",
    "            \"boxes\": torch.tensor([xmin, ymin, xmax, ymax]),\n",
    "            \"labels\": torch.tensor([p.argmax()]),\n",
    "            \"scores\": torch.tensor([p.max()]),\n",
    "        }\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # create a CocoEvaluator\n",
    "    coco_evaluator = CocoEvaluator(data_loader.dataset.coco, iou_types=[\"bbox\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) if torch.is_tensor(v) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # get the model's predictions\n",
    "            predictions = model(images)\n",
    "\n",
    "            # convert predictions to the correct format\n",
    "            predictions = {target[\"image_id\"].item(): prediction for target, prediction in zip(targets, predictions)}\n",
    "\n",
    "            # update the evaluator with the model's predictions\n",
    "            coco_evaluator.update(predictions)\n",
    "\n",
    "    # compute the metrics\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "\n",
    "    # get the mAP and mAR\n",
    "    stats = coco_evaluator.coco_eval['bbox'].stats\n",
    "    mAP = stats[0:5].mean()\n",
    "    mAR = stats[5:10].mean()\n",
    "\n",
    "    return mAP, mAR\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def evaluate_detr(model, data_loader, device):\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # create a CocoEvaluator\n",
    "    coco_evaluator = CocoEvaluator(data_loader.dataset.coco, iou_types=[\"bbox\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) if torch.is_tensor(v) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # get the model's predictions\n",
    "            predictions = [detect(image, model, transform) for image in images]\n",
    "\n",
    "            # convert predictions to the correct format\n",
    "            predictions = {target[\"image_id\"].item(): prediction for target, prediction in zip(targets, predictions)}\n",
    "\n",
    "            # update the evaluator with the model's predictions\n",
    "            coco_evaluator.update(predictions)\n",
    "\n",
    "    # compute the metrics\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "\n",
    "    # get the mAP and mAR\n",
    "    stats = coco_evaluator.coco_eval['bbox'].stats\n",
    "    mAP = stats[0:5].mean()\n",
    "    mAR = stats[5:10].mean()\n",
    "\n",
    "    return mAP, mAR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/gijsf/.cache/torch/hub/facebookresearch_detr_main\n"
     ]
    }
   ],
   "source": [
    "transform_image, transform_target = get_transform(train=True)\n",
    "\n",
    "test_dataset_COCO = COCO('../data/annotations/test_labels.json')\n",
    "coco_dataset = CocoDetection(root='../data/images/', annFile='../data/annotations/test_labels.json', transform=T.ToTensor())\n",
    "data_loader = DataLoader(coco_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "resnet = get_model_resnet(41, 'resnet.pth')\n",
    "resnet.to(device)\n",
    "\n",
    "mobilenet = get_model_mobilenet(41, 'mobilenet.pth')\n",
    "mobilenet.to(device)\n",
    "\n",
    "detr = get_model_detr('detr.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gijsf/miniconda3/envs/Im3D/lib/python3.6/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811792945/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating evaluation results...\n",
      "DONE (t=0.99s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.188\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.312\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.194\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.466\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.192\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.467\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.483\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.354\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.472\n",
      "ResNet: mAP = 0.26473533374720126, mAR = 0.3920120934050683\n"
     ]
    }
   ],
   "source": [
    "mAP_resnet, mAR_resnet = evaluate_model(resnet, data_loader, device)\n",
    "print(f\"ResNet: mAP = {mAP_resnet}, mAR = {mAR_resnet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating evaluation results...\n",
      "DONE (t=0.82s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.108\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.188\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.107\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.279\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.108\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.108\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.206\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.271\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.274\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.286\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.260\n",
      "MobileNet: mAP = 0.15795107185606666, mAR = 0.2290970658615749\n"
     ]
    }
   ],
   "source": [
    "mAP_mobilenet, mAR_mobilenet = evaluate_model(mobilenet, data_loader, device)\n",
    "print(f\"MobileNet: mAP = {mAP_mobilenet}, mAR = {mAR_mobilenet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-acd0ffd85489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmAP_detr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmAR_detr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_detr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"DETR: mAP = {mAP_detr}, mAR = {mAR_detr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-ac8782f99a02>\u001b[0m in \u001b[0;36mevaluate_detr\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# update the evaluator with the model's predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mcoco_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# compute the metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis/2D/all/lib/coco_eval.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, predictions)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miou_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miou_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mredirect_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mcoco_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadRes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis/2D/all/lib/coco_eval.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, predictions, iou_type)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miou_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_for_coco_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miou_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"segm\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_for_coco_segmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis/2D/all/lib/coco_eval.py\u001b[0m in \u001b[0;36mprepare_for_coco_detection\u001b[0;34m(self, predictions)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_xywh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "mAP_detr, mAR_detr = evaluate_detr(detr, data_loader, device)\n",
    "print(f\"DETR: mAP = {mAP_detr}, mAR = {mAR_detr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Im3D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
